{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Benchmarking\n",
    "\n",
    "In this notebook we will show how you can use CARLA for benchmarking. First, we need to load some factuals, and generate counterfactuals for them. For more explanation on how to do this, please take a look at our [How to use CARLA](https://carla-counterfactual-and-recourse-library.readthedocs.io/en/chore-update_documentation/notebooks/how_to_use_carla.html) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Python-MIP package version 1.12.0 [model.py <module>]\n"
     ]
    }
   ],
   "source": [
    "from carla import Benchmark\n",
    "import carla.evaluation.catalog as evaluation_catalog\n",
    "from carla.data.catalog import OnlineCatalog\n",
    "from carla.models.catalog import MLModelCatalog\n",
    "from carla.models.negative_instances import predict_negative_instances\n",
    "import carla.recourse_methods.catalog as recourse_catalog\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Generating counterfactuals\n",
    "\n",
    "Before we can benchmark anything, we need some data, a classification model and a recourse method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_name = \"adult\"\n",
    "dataset = OnlineCatalog(data_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load catalog model\n",
    "model_type = \"ann\"\n",
    "ml_model = MLModelCatalog(\n",
    "    dataset,\n",
    "    model_type=model_type,\n",
    "    load_online=True,\n",
    "    backend=\"pytorch\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Start training of Variational Autoencoder... [models.py fit]\n",
      "[INFO] [Epoch: 0/5] [objective: 0.375] [models.py fit]\n",
      "[INFO] [ELBO train: 0.38] [models.py fit]\n",
      "[INFO] [ELBO train: 0.13] [models.py fit]\n",
      "[INFO] [ELBO train: 0.12] [models.py fit]\n",
      "[INFO] [ELBO train: 0.12] [models.py fit]\n",
      "[INFO] [ELBO train: 0.12] [models.py fit]\n",
      "[INFO] ... finished training of Variational Autoencoder. [models.py fit]\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\n",
    "        \"data_name\": data_name,\n",
    "        \"vae_params\": {\n",
    "            \"layers\": [len(ml_model.feature_input_order), 512, 256, 8],\n",
    "        },\n",
    "    }\n",
    "\n",
    "# define your recourse method\n",
    "recourse_method = recourse_catalog.CCHVAE(ml_model, hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# get some negative instances\n",
    "factuals = predict_negative_instances(ml_model, dataset.df)\n",
    "factuals = factuals[:100]\n",
    "\n",
    "# find counterfactuals\n",
    "counterfactuals = recourse_method.get_counterfactuals(factuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "   y-Nearest-Neighbours  L0_distance  L1_distance  L2_distance  Linf_distance  \\\n0                  0.21         10.0     4.348742     4.036693            1.0   \n1                   NaN          8.0     2.235326     2.016212            1.0   \n2                   NaN          7.0     1.564834     1.106475            1.0   \n3                   NaN         11.0     5.938143     5.252787            1.0   \n4                   NaN          8.0     2.435960     2.054773            1.0   \n\n   Success_Rate  Redundancy  Constraint_Violation  avg_time  \n0           1.0           6                     1   1.52311  \n1           NaN           3                     1       NaN  \n2           NaN           5                     1       NaN  \n3           NaN           6                     2       NaN  \n4           NaN           6                     1       NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>y-Nearest-Neighbours</th>\n      <th>L0_distance</th>\n      <th>L1_distance</th>\n      <th>L2_distance</th>\n      <th>Linf_distance</th>\n      <th>Success_Rate</th>\n      <th>Redundancy</th>\n      <th>Constraint_Violation</th>\n      <th>avg_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.21</td>\n      <td>10.0</td>\n      <td>4.348742</td>\n      <td>4.036693</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>6</td>\n      <td>1</td>\n      <td>1.52311</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>2.235326</td>\n      <td>2.016212</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>7.0</td>\n      <td>1.564834</td>\n      <td>1.106475</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>11.0</td>\n      <td>5.938143</td>\n      <td>5.252787</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>2</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>8.0</td>\n      <td>2.435960</td>\n      <td>2.054773</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>6</td>\n      <td>1</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# first initialize the benchmarking class by passing\n",
    "# black-box-model, recourse method, and factuals into it\n",
    "benchmark = Benchmark(ml_model, recourse_method, factuals)\n",
    "\n",
    "# now you can decide if you want to run all measurements\n",
    "# or just specific ones.\n",
    "evaluation_measures = [\n",
    "    evaluation_catalog.YNN(benchmark.mlmodel, {\"y\": 5, \"cf_label\": 1}),\n",
    "    evaluation_catalog.Distance(benchmark.mlmodel),\n",
    "    evaluation_catalog.SuccessRate(),\n",
    "    evaluation_catalog.Redundancy(benchmark.mlmodel, {\"cf_label\": 1}),\n",
    "    evaluation_catalog.ConstraintViolation(benchmark.mlmodel),\n",
    "    evaluation_catalog.AvgTime({\"time\": benchmark.timer}),\n",
    "]\n",
    "\n",
    "# now run all implemented measurements and create a\n",
    "# DataFrame which consists of all results\n",
    "results = benchmark.run_benchmark(evaluation_measures)\n",
    "\n",
    "display(results.head(5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
